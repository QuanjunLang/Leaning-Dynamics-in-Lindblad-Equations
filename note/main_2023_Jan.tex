\documentclass[10pt]{article}  % [12pt] option for the benefit of aging markers
\usepackage{amssymb}
\usepackage{amsthm}    % amssymb package contains more mathematical symbols
\usepackage{graphicx}          % graphicx package enables you to paste in graphics
\usepackage{amsmath}         % best maths package on offer
\usepackage{mathtools}


%\mathtoolsset{showonlyrefs}		% Only show the label of equations that have been referenced

\usepackage{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{listings}
\usepackage{titlepic}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{enumerate}
\usepackage{fontspec}
\usepackage{mathalfa}
\usepackage{todonotes}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{showlabels}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\usepackage{braket}
\usepackage{physics}

\usepackage[
backend=bibtex,
style=alphabetic,
isbn=false,
%editor=false,
]{biblatex}
\addbibresource{Lindblad.bib}


\DeclareSourcemap{
  \maps[datatype=bibtex, overwrite]{
    \map{
      \step[fieldset=address, null]
      \step[fieldset=editor, null]
      \step[fieldset=location, null]
    }
  }
}


%\usepackage[style=ieee]{biblatex}
\AtEveryBibitem{%
  	\clearfield{issn} % Remove issn
  	\clearfield{doi} % Remove doi
  	\clearfield{urldate}
	\clearfield{month}
	\clearfield{adress}
  	\ifentrytype{online}{}{% Remove url except for @online
    \clearfield{url}
  }
}
%\renewbibmacro*{url+urldate}{}
%\renewbibmacro*{editor}{}
%\renewbibmacro*{editors}{}
%\renewbibmacro{in:}{}



\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor = blue,
}

\geometry{left = 2.5cm, right=2.5cm, top=1cm, bottom=1.5cm}

%%%%%%%%%%%%%% symbols %%%%%%%%%%%%%
\newcommand{\eps}{\varepsilon}
%%%%%%%%%%%%%%%%%%%% Theorem Environments %%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{Lem}[thm]{Lemma}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{example}[thm]{Example}
\newtheorem{ex}[thm]{Example}
\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%% bold letters %%%%%%%%%
\newcommand{\bSig}{\boldsymbol{\Sigma}}
\newcommand{\bLam}{\boldsymbol{\Lambda}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bdeta}{\boldsymbol{\eta}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bxi}{\boldsymbol{\xi}}
%%%%%%%%%%%%%%%%%% fonts %%%%%%%%%%%%%%%%
\newcommand{\vect}[1] {\pmb{#1}}
\newcommand{\mat}[1]{\pmb{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%%%%%%%%%%%%% brackets %%%%%%%%%%%%%%
\newcommand{\rbracket}[1]{\left(#1\right)}      %round
\newcommand{\sbracket}[1]{\left[#1\right]}      %square    
\newcommand{\cbracket}[1]{\left\{#1\right\}}      %curve
%\newcommand{\norm}[1]{\left\|#1\right\|}
%\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\innerp}[1]{\langle{#1}\rangle}
\newcommand{\dbinnerp}[1]{\langle\hspace{-1mm}\langle{#1}\rangle \hspace{-1mm}\rangle}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
%%%%%%%%%%%%%%%%%%% mathcal %%%%%%%%%%%%%
\def\mH{\mathcal{H}}
\def\mL{\mathcal{L}}
\def\mE{\mathcal{E}}
\def\mS{\mathcal{S}}
\def\mN{\mathcal{N}}
\def\mD{\mathcal{D}}
\def\mA{\mathcal{A}}
\def\mV{\mathcal{V}}
\def\mB{\mathcal{B}}
\def\mR{\mathcal{R}}
\def\mT{\mathcal{T}}

%%%%%%%%%%%%%%%%%% words operation %%%%%%%
%\def\supp{\mathrm{supp}}
%\def\span{\mathrm{span}}
\def\div{\mathrm{div}}
%%%%%%%%%%%%%%%%%% mathbb %%%%%%%%%%%%%      
\def\Rn{\mathbb{R}^n}
\def\R{\mathbb{R}}          
\def\C{\mathbb{C}}                       
\def\P{\mathbb{P}}
%%%%%%%%%%%%%%%%% mathbf %%%%%%%%%%%%%%
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bfm}{\mathbf{m}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bDD}{\mathbf{D}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\wbH}{\widetilde{\mathbf{H}}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\boldeta}{\boldsymbol{\eta}}
\newcommand{\boldlambda}{\boldsymbol{\lambda}}
\newcommand{\boldalpha}{\boldsymbol{\alpha}}
\newcommand{\boldxi}{\boldsymbol{\xi}}
\newcommand{\boldphi}{\boldsymbol{\varphi}}
%%%%%%%%%%% text %%%%%%%%%%%%%%%
\newcommand{\Log}{\text{Log}}
\newcommand{\diag}{\text{diag}}
\renewcommand{\vec}{\textbf{vec}}
%%%%%%%%%%%%%% comments %%%%%%%%%%%%%%%
\newcommand{\QL}[1]{\textcolor{cyan}{{#1}}}
\newcommand{\jl}[1]{\textcolor{red}{[\textsf{JL: #1]}}}
\newcommand{\bl}[1]{{\color{magenta} [\textbf{BL}: #1]}}

% for bwl
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\maf}[1]{\mathfrak{#1}}
\newcommand{\ms}[1]{\mathscr{#1}}
\renewcommand{\R}{\mathbb{R}} 
\renewcommand{\C}{\mathbb{C}} 
\newcommand{\lad}{\lambda}
\newcommand{\si}{\sigma}
\newcommand{\vr}{\varrho}
\newcommand{\rd}{\mathrm{d}}
\newcommand{\magg}[1]{{\color{magenta} #1}}
\def \ww {\omega}
\def \d {\delta}
\def \h {\hat} 
\def \ep {\varepsilon}
\def \w {\widetilde}
\def \q {\quad}
\def \id {{\rm id}}
\def  \mi {{\bf 1}}
\def \bh {\mc{B}(\mc{H})}
\def \l {\langle}
\def \r {\rangle}
\def \si {\sigma}
\def \dd {\cdot}
\def \pa {\partial}


\title{Efficient estimation of Lindbladians from trajectory data}
\author{Quanjun Lang, Bowen Li, Jianfeng Lu}
\date{}

\begin{document}

\maketitle

\tableofcontents
\section{Introduction}

We study the problem of learning in open quantum systems. The goal is to learn the dynamics in the Lindblad quantum master equation (QME)
\begin{equation}\label{eq_QME_main}
	\frac{d}{dt} \rho = -i[H, \rho] + \sum_{k = 1}^{N_J} (J_k \rho J_k^\dagger - \frac{1}{2} J_k^\dagger J_k \rho - \frac{1}{2} J_k^\dagger J_k \rho)
\end{equation}
from observations of the trajectories. Here $H$ is the system Hamiltonian and $J_k$'s are jump operators for $k = 1, \dots N_J$. The density matrix $\rho \in \C^{N \times N}$ here characterizes the distribution of a quantum $\ket{\psi(t)}$ in the sense that $\mathbb{E}[\ketbra{\psi(t)}{\psi(t)}]$. 
Here $\ket{\psi(t)} \in \C^N$ itself satisfies the stochastic Schr\"{o}dinger equation
\begin{equation}
	d \ket{\psi(t)} = \left( -i H \ket{\psi(t)} - \frac{1}{2}\sum_{k = 1}^r J_k^\dagger J_k \ket{\psi(t)}\right) dt + \sum_{k = 1}^r J_k \ket{\psi(t)} dW^k_t
\end{equation}
where $W^k_t$'s are independent Brownian motions.


The above QME can be written as
\begin{align}\label{eq_QME_Lindbladian}
	\frac{d}{dt} \rho := \mL \rho =  \mL_H\rho + \mL_J \rho, 
\end{align}
where $\mL_H \rho = -i[H, \rho]$ and $\mL_J \rho = \sum_{k = 1}^{N_J} (J_k \rho J_k^\dagger - \frac{1}{2} J_k^\dagger J_k \rho - \frac{1}{2} J_k^\dagger J_k \rho)$. The super operator $\mL$ is called the Lindbladian, and solutions of \eqref{eq_QME_Lindbladian} can be expressed using semigroup generated by $\mL$, namely
\begin{equation}
	\rho(t) = e^{\mL t}\rho(0).
\end{equation}
Then the map $\mV(t) = e^{\mL t}$ is a complete positive trace-preserving (CPT) map for arbitrary time $t$. By the Choi-Kraus' Theorem, a mapping $\mV:\mB(\R^n) \to \mB(\R^n)$ is completely positive and trace-preserving if and only if it can be expressed as 
\begin{equation}
	\mV\rho = \sum_{k}A_k^\dagger \rho A_k
\end{equation}
where $\sum_{k}A_kA_k^\dagger = I_\mH$, and $\mB(H)$ represents the space of bounded operators on $\R^n$. 




The possible measurements of the density $\rho$ are described by a Hermitian operator or $observable$ $O$. The observation result is achieved by taking the expectation value of the outcome, which gives 
\begin{equation}
	\expval{O}:= \tr(O\rho).
\end{equation}
Using observables to infer the density $\rho$ is called Quantum state tomography. As we will point out later, the full state information makes the estimation of $H$ and $J_k$ more efficient. 

We aim to learn the Hamiltonian $H$ and the jump operators $J_k$. We will first assume a discrete observation of density $\rho(t_l)^{(m)}$ from $M$ independent copies, where $t_l = l\Delta t, l = 0, 1, \dots, N_T$ is assumed to be an equidistant time grid, with $T:=N_T \Delta t$.  The problem is concluded as the following. 

\begin{equation}
	\textbf{Data: $\{\rho(t_l)^{(m)}\}_{m = 1, l = 1}^{M, N_T}$}; \ \ \textbf{Goal: $\{H, J_1, \dots, J_{N_J}\}$}
\end{equation}



\subsection*{Related works} 
\subsubsection*{Quantum channel tomography}
\bl{Literature review for Lindbladian learning/Prony method for system eigenvalues/Matrix completion}

%\subsection*{Difficulty and contribution} 
%Firstly, the evaluation of the density requires taking the expectation with respect to an observable, namely
%$$ \innerp{A, \rho}_F = \tr(A \rho) = \expval{A}$$
%where $A = \sum_i a_i \ketbra{a_i}{a_i}$. When observe using $A$, we obtain state $\ket{a_i}$ with probability $a_i$. In practice, we take the empirical distribution of the observed states as the estimated value of $\tr(A, \rho)$. Therefore, the accuracy suffers from the law of large numbers with order $1/\sqrt{N}$, where $N$ is the number of independent trials. With given observable $A$, the original equation \eqref{eq_QME_main} can be written as 
%\begin{equation}
%	\frac{d}{dt} \expval{A}(t) = -i\expval{[A, H]}(t) + \sum_{k = 1}^r \left(\expval{C_k^\dagger A C_k}(t) - \frac{1}{2} \expval{A C_k^\dagger C_k} (t) - \frac{1}{2} \expval{C_k^\dagger C_k A}(t) \right)
%\end{equation}
%
%Due to the expense of evaluating the states, the derivative term in \eqref{eq_QME_main} is unrealistic to obtain. Since the finite difference 
%$$ \frac{\expval{A}(t + \Delta t) - \expval{A}(t)}{\Delta t} $$ 
%will amplify the error in $\expval{A}(t)$ and $\expval{A}(t + \Delta t)$. The error in the derivative is of order $\Delta t^2 + \frac{\delta}{\Delta t}$, where the first $\Delta t^2 $ is the error in finite difference using midpoint method, and the $\delta$ is the accuracy in the expectation. In order to achieve $\varepsilon$ accuracy in the derivative, we need $\Delta t \leq \sqrt{\varepsilon}$ and both $\expval{A}(t)$ and $\expval{A}(t + \Delta t)$ to have $\delta = \varepsilon^{3/2}$ accuracy, therefore requires $1/\varepsilon^{3/2}$ number of independent trails in evaluating the expectation.

\subsection*{Notation and outline}
We fix the notation used throughout this work. Let $\mc{H}\simeq \C^N$ be a finite-dimensional Hilbert space with $N = 2^n$ and $\mc{B}(\mc{H})$ be the space of bounded operators. We denote by $\mc{B}_{sa}(\mc{H})$ the subspace of self-adjoint operators and by $\mc{B}^{+}_{sa}(\mc{H})$ the cone of positive semidefinite operators. For simplicity, we write $A \ge 0$ (resp., $A > 0$) for a positive semidefinite (resp., definite) operator. We then denote by $\mc{D}(\mc{H}) := \{\rho \in \mc{B}_{sa}^+(\mc{H})\,;\  \tr \rho =1 \}$ the set of density operators (quantum states), and by $\mc{D}_+(\mc{H})$ the full-rank density operators. Moreover, let $X^*$ be the adjoint operator of $X$ and $\l X, Y\r = \tr (X^*Y)$ be the Hilbert-Schmidt inner product on $\bh$. 
The adjoint of a superoperator $\Phi: \mc{B}(\mc{H}) \to \mc{B}(\mc{H})$ with respect to the inner product $\l \dd,\dd \r $ is denoted by $\Phi^\dag$. 

\begin{table}[!t] 
{\small
\begin{center} 
\caption{ \, Notations} \label{tab:notation}
\begin{tabular}{ l  l }
\toprule % \hline
Notation   		&  Description \\  \hline
$\mL$    		& true and generic Lindbladian operator\\
$H$    	 		& true and generic Hamiltonian operator\\
$J_k$    		& true and generic jump operator \\
$\rho$			& density matrix\\
$C = (c_{jk}) = UU^\dagger$ 	& Kossakowski matrix and its low rank decomposition\\
$\gamma_k$ 		& Eigenvalues of Kossakowski matrix\\
$\{F_i\} = (\{S^{(j, k)}\}, \{J^{(j, k)}\}, \{D^l\})$	& Orthonormal basis of Hermitian traceless matrices\\
$h_j = \tr(HF_j), h_j \in \R$			& coordinate of Hamiltonian on basis $\{F_j\}$\\
$g_j(\rho) = -i[F_j, \rho]$ & coefficient of Hamiltonian matrix\\
$G_{k, l}(\rho) = \frac{1}{2}[F_k, \rho F_l^\dagger] + [F_k \rho, F_l^\dagger]$ & coefficient of Kossakowski matrix\\
\hline 
$\vec:\C^{N\times N} \to \C^{N^2}$  		& vectorization operator\\
$\mR: \C^{N \times N} \to \C^{N \times N}$ & reshape operator\\
\hline 
$\bL = \sum_{k = 1}^{N_K} V_k \otimes U_k$				& Lindbladian operator on the vectorized density matrices\\
$\bE = \mR(\bL) = \sum_{k = 1}^{N_K} \vec(V_k) \vec^\top(U_k) = \bV\bU^\top$ 	& reshaped Lindbladian operator on the vectorized density matrices\\
$\bU = (\vec(U_1)^\top,\dots, \vec(U_{N_K})^\top)^\top$ & \\
$\bV = (\vec(V_1)^\top,\dots, \vec(V_{N_K})^\top)^\top$ & low rank decomposition of $\bE$\\
\hline 
$N$   & dimension of the density matrix\\
$N_J$ & number of jump operators in Lindbladian $\mL$\\
$N_K$ & number of Kronecker products in reshaped Lindbladian $\mR(\bL)$.\\
\bottomrule	
\end{tabular}  
\end{center}
}
\end{table}


\section{Preliminary}

\subsection{Quantum Markovian dynamics}

\subsection{Observable and quantum state tomography}

\subsection{Pauli basis and Kossakowski matrix}
We can construct the orthonormal basis of the space of traceless matrices which consists of three types, $\{F_i\} = (\{S^{(j, k)}\}, \{J^{(j, k)}\}, \{D^l\})$. Let $e_j$ denote the $j$-th standard basis vector in $\C^N$ and $E^{(j, k)}:= e_j e_k^\top$ denote the matrix in $\C^{N \times N}$ with all entries being zeros, except for the $(j, k)$-th being one. For $j \neq k$, there are $N(N-1)$ basis matrices, given by
\begin{align}
	S^{(j, k)} &= \frac{1}{\sqrt{2}}\rbracket{E^{(j, k)} + E^{(k, j)}}, 1 \leq j < k \leq N\\
	J^{(j, k)} &= \frac{-i}{\sqrt{2}}\rbracket{E^{(j, k)} - E^{(k, j)}}, 1 \leq j < k \leq N
\end{align}
For the case of $j = k$, we construct $N-1$ matrices explicitly with the traceless constraint. 
\begin{align}
	D^{(l)} = \frac{1}{\sqrt{l(l+1)}} \rbracket{\sum_{k = 1}^l E^{(k, k)} - lE^{(l+1, l+1)}}, l = 1, \dots, N-1
\end{align}
Given the orthonormal basis $\{F_i\}$, we have the following theorem. 

\begin{theorem}
A linear operator $\mL: \C^{N\times N} \to \C^{N\times N}$ is the generator of a completely positive semigroup of $\C^{N \times N}$ if it can be expressed in the following form
\begin{equation}\label{eq_GKS}
	\mL \rho = -i[H, \rho] + \frac{1}{2} \sum_{j, k = 1}^{N^2-1}c_{kl}\rbracket{[F_k, \rho F_l^*] + [F_k \rho, F_l^*]}
\end{equation}
where $H = H^*, \tr(H) = 0, \tr(F_k) = 0, \tr(F_jF_k^*) = \delta_{jk}, j, k = 1,2, \dots, N^2-1$, and $C = (c_{jk})$ is a complex positive matrix. 
\end{theorem}
The above form is often called the GKS (Gorini-Kossakowski-Sudarshan) form of the generator $\mL$ and $C$ the Kossakowski matrix. Since the matrix $C$ is positive, it can be diagonalized with a unitary matrix $U$, therefore 
\begin{equation}
	UCU^\dagger = \diag(\gamma_1, \dots, \gamma_{N^2-1})
\end{equation}
where the eigenvalues $\gamma_i$ are non-negative. The jump operators can be defined through 
\begin{equation}
	F_j = \sum_{k = 1}^{N^2 - 1}u_{kj}A_k.
\end{equation}
Note that $A_k$ orthonormal, i.e. $\tr(A_kA_l) = \delta_{kl}$. We have the following diagonal form
\begin{equation}
	L\rho = -i[H, \rho] + \sum_{k = 1}^{N^2 - 1}\gamma_k \rbracket{A_k\rho A_k - \frac{1}{2}A_k^\dagger A_k \rho - \frac{1}{2}A_k^\dagger A_k}.
\end{equation}
Let $J_k = \sqrt{\gamma_k}A_k$, we recover the form of Lindblad equation as in \eqref{eq_QME_main}. Note that the generator is invariant under the following transformations
\begin{itemize}
	\item Unitary transformations of the set of jump operators,
	$$ \sqrt{\gamma_i}A_i \to \sqrt{\gamma_i'}A_i' = \sum_{j = 1}v_{ij}\sqrt{\gamma_j}A_j$$
	where $v_{ij}$ is a unitary matrix.
	\item Inhomogeneous transformations
	\begin{align}
		A_i &\to A_i' = A_i + a_i\\
		H &\to H' = H + \frac{1}{2i}\sum_j \gamma_j\rbracket{\overline{a_j}A_j - a_jA_j^\dagger} + b,
	\end{align}
	where $a_i$ are complex numbers and $b$ is real.
\end{itemize}

By the above transformations, it is always possible to choose traceless Hamiltonian and traceless orthogonal jump operators. For the Hamiltonian $H$, since it is a Hermitian matrix
\begin{equation}
	h_i:=\tr(HF_i)
\end{equation}
is a real number for all basis matrices $F_i$. By the transformation condition 1, we cannot uniquely determine jump operators. Together with the low rank assumption that $N_J \ll N^2-1$, we aim to find the following unique representation of the Lindbladian, 
\begin{align}
	&\{h_i\}_{i = 1}^{N^2-1}, &h_i \in \R \\
	&C\in\C^{(N^2-1)\times(N^2-1)}, &rank(C) = N_J
\end{align}
Note that $C$ is assumed to be Hermitian, therefore we can find matrix $U \in \C^{(N^2-1)\times N_J}$ such that $C = UU^\dagger.$

In this case, the Lindbladian can be written as 
\begin{equation}
	\mL \rho = \sum_{j= 1}^{N^2-1}h_j g_j(\rho) + \sum_{k, l = 1}^{N^2-1} c_{k, l}G_{k, l}(\rho)
\end{equation}
where $g_j(\rho) = -i[F_j, \rho]$ and $G_{k, l}(\rho) = \frac{1}{2}[F_k, \rho F_l^\dagger] + [F_k \rho, F_l^\dagger]$.

\section{A framework of learning Lindbladian}
The given data is assumed to be observed on a sparse time grid. To estimate the derivative of the trajectory, the first step is to use the Prony method for each entry of the density $\rho$. We then use the alternating least squares to reconstruct the Hamiltonian and jump operators by exploiting its low-rank structure.



\subsection{Prony fitting of the trajectory}
\bl{We use Prony method to estimate the spectrum of the system and the derivative of the trajectory. We then use the alternating least squares to reconstruct the Lindbladian by exploiting its low-rank structure.
}
To increase the accuracy of the estimation of the derivatives, we use the Prony method to fit the trajectories $\expval{A}$. Since the solution can be written as $\rho(t) = e^{t\bL } \rho(0)$, and $\bL$ contains the jump operators, it converges to a steady state exponentially. In the case the $\bL$ is diagonalizable, $\bL = U \Sigma U^{-1}$, where $\Sigma = \diag\{\lambda_1, \dots, \lambda_{n^2}\}$(possibly repeated). Then 
\begin{equation}
	\rho(t) = Ue^{t\Sigma}U^{-1}\rho(0)
\end{equation}



\subsection{Loss function}
Assume that the initial density $\rho(0)$ follows a distribution $\mu_0$, and then we can define the loss function as
\begin{equation}
	\mE(\widehat{\mL}) = \mathbb{E}_{\mu_0}\left[\int_0^T \norm{\dot{\rho}_t - \widehat{\mL}\rho_t }_F^2 dt\right]
\end{equation}
where $\norm{A}_F$ represents the Frobenius norm of $A$ and $\widehat{\mL}$ represents a candidate Lindbladian. 
Given $M$ independent density trajectory observed on a uniform discrete mesh $\{t_l\}_{l = 1}^L$, $t_l = l \Delta t$, namely the following data
\begin{equation}
	\textbf{Data:} \{\rho^m_{t_l}\}_{m, l}^{M, L}
\end{equation}
We also assume a Gaussian random noise for each observation point.  Given the observation data, it is natural to define the loss that approximates the above integral since it represents the negative log-likelihood. Namely, we have 
\begin{equation}
	\mE(\widehat{\mL}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L \norm{\dot{\rho}^{m}_{t_l} - \widehat{\mL}\rho^m_{t_l} }_F^2
\end{equation}

\subsection{One stage algorithm}
Note that the candidate Lindbladian is determined by $\widehat{\mL}$ as $(\widehat{h}, \widehat{U})$, we have the loss functions
\begin{equation}
	\mE(\widehat{h}, \widehat{C}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L \norm{\dot{\rho}^{m}_{t_l} - \rbracket{\sum_{j}\widehat{h_j}g_j(\rho^m_{t_l}) + \sum_{k, l}\widehat c_{kl}G_{kl}(\rho^m_{t_l})}}_F^2
\end{equation}
Note that the loss is quadratic in $h$, but we have not utilize the low rank property of $\widehat{C}$. By taking $\widehat{C} = \widehat{U}\widehat{U}^\dagger$, the loss becomes quartic in $\widehat{U}$. From the later algorithm, we relax the condition of $\widehat{C} = \widehat{U}\widehat{U}^\dagger$ to be $\widehat{C} = \widehat{U}\widehat{V}^\dagger$ and add an extra constraint $\|\widehat{U} - \widehat{V}\|_F^2$. Therefore the loss becomes 
\begin{equation}
	\mE(\widehat{h}, \widehat{U}, \widehat{V}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L \norm{\dot{\rho}^{m}_{t_l} - \rbracket{\sum_{j}\widehat{h_j}g_j(\rho^m_{t_l}) + \sum_{k, j, l}\widehat u_{k, j}\widehat v^\dagger_{j, l}G_{kl}(\rho^m_{t_l})}}_F^2	+ \lambda\|\widehat{U} - \widehat{V}\|_F^2
\end{equation}


\subsubsection{Alternating Least Squares}
We use the alternating least squares (ALS) to minimize the loss function $\mE$.   For the triplet $(\widehat{h}, \widehat{U}, \widehat{V})$, the loss functions is quadratic for each one if the other two terms are fixed. It is direct to construct linear regression. When $N$ is large, we can apply gradient descent for each step of the least square. 



\subsubsection{Connection to Matrix sensing}
In the language of matrix sensing, the matrix $P^m_{t_l, ij}$ can be considered as the measurement matrices, with the corresponding output $(\rho^m_{t_l})_{ij}$. Now we define the operator $\mA : \R^{n^2\times n^2}\to \R^{MLn^2}$ as
\begin{equation}
	\mA(\widehat{\bE}) = \vec\left[(\innerp{P^m_{t_l, ij}, \widehat{\bE}}_F)_{m, l, i, j}\right]
\end{equation}
and the vector $\by\in \R^{MLn^2}$ as $\vec\left[(\rho^m_{t_l})_{ij}\right]$, we have the loss function written as 
\begin{equation}
	\mE(\widehat{E}) = \norm{\mA(\widehat{\bE}) - \by}^2
\end{equation}
up to a constant. 



\subsection{Two stage algorithm}
The previous method refers to estimate the parameters of the Lindbladian operator over the basis matrices $\{F_i\}$, which guaranteed the uniqueness. But the computation of the projection of $\rho$ on to the basis can be expensive, therefore we introduce the two stage algorithm, which refers to exploiting the low rank structure of the Lindbladian operator first, and then perform the decomposition over the basis $\{F_i\}$. 
\subsubsection{Vectorization}
Using the fact that 
\begin{equation}
	\vec(AXB) = (B^\top \otimes A)\vec(X)
\end{equation}
where $\vec$ is the vectorization of a matrix in $\C^{N\times N}$  to a vector in $\C^{N^2}$ in the column-first order, we can write the vectorized version of the equation \eqref{eq_QME_main}.
\begin{equation}
	\vec(\rho)'(t) = \left\{-i(I\otimes H - H^\top \otimes I) + \sum_{k = 1}^{N_J} \left[\overline{J_k}\otimes J_k -\frac{1}{2} I\otimes (J_k^\dag J_k) - \frac{1}{2} \overline{(J_k^\dag J_k)}\otimes I\right]\right\}\vec(\rho)(t) := \bL \ \vec(\rho)(t).
\end{equation}
where the super operator is denoted as $\bL$. Note that $\bL$ has only $N_K = 2+3N_J$ Kronecker products in the summation, where the subscript $K$ represents the number of Kronecker terms. We can write $\bL$ as 
\begin{equation}
	\bL = \sum_{k = 1}^{N_K} V_k \otimes U_k
\end{equation}
where $V_k$ and $U_k$ are matrices in $\C^{N\times N}$. And the corresponding operator $\mL$ is written as 
\begin{equation}
	\mL \rho = \sum_{k = 1}^{N_K}	U_k \rho V_k^\top
\end{equation}
We denote the map $\mT: \mL(\C^{N\times N})\to \C^{N^2\times N^2}$ which maps $\mL$ to $\bL$. Note that $\mT$ is induced by the $\vec$ operator, 
\begin{equation}
	\mT(\rho) = \bL \,\vec(\rho)
\end{equation}
for all $\rho \in \C^{N\times N}$.

\subsubsection{Reshaping}
We apply the reshaping operator $\mR$, defined as in \cite{vanloanApproximationKroneckerProducts1993}. If matrix $A \in \C^{N^2\times N^2}$, define the rearrangement of $A$ (relative to the blocking parameters $N$) by
\begin{equation}
	A = 
	\begin{bmatrix}
 	A_{11} \dots A_{1n}\\
 	\vdots \ddots \vdots\\
 	A_{n1} \dots A_{nn} 
 	\end{bmatrix}, 
 	\quad
	\mR(A) = 
	\begin{bmatrix}
		\widetilde A_1\\ \vdots \\ \widetilde A_{n}	
	\end{bmatrix}
	,\quad 
	\widetilde A_j = 
	\begin{bmatrix}
		\vec(A_{1j})^\top \\ 
		\vdots \\
		\vec(A_{nj})^\top	
	\end{bmatrix}
\end{equation}
This rearrangement operator $\mR$ gives 
\begin{equation}
	\mR(A\otimes B) = \vec(A) \vec^\top(B),
\end{equation}
Therefore, 
\begin{equation}
	\bE := \mR(\bL) = \sum_{k = 1}^{N_K} \vec(V_k) \vec^\top(U_k) = \bV \bU^\top
\end{equation}
That is to say, the matrix $\bE$ has low rank $N_K$ compared to $N^2$. We denote 
\begin{equation}
	\bU = 
	\begin{bmatrix}
		\vec(U_1)\\ \vdots \\ \vec(U_{N_K})
	\end{bmatrix}, \quad
	\bV = 
	\begin{bmatrix}
		\vec(V_1)\\ \vdots \\ \vec(V_{N_K})
	\end{bmatrix}
\end{equation}
where $\bU, \bV \in \C^{N \times N_K}$ to be the decomposition of $\bE$. 

In order to use the rearranged low-rank property, we vectorize the density matrix $\rho$ and decompose the Frobenius norm as 
\begin{equation}
	\mE(\widehat{\mL}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L\sum_{i, j = 1}^N \abs{(\dot{\rho}^{m}_{t_l})_{i, j } - (\widehat{\mL}\rho^m_{t_l})_{i, j}}^2.
\end{equation}
Notice that 
\begin{equation}
	(U_k\rho V_k^\top)_{i, j} = \vec(U_k)^\top P_{ij} \vec(V_k) = \innerp{P_{ij}, \vec(U_k)\vec(V_k)^\top}_F
\end{equation}
where $P_{ij} = E^{(i, j)} \otimes \rho$, and recall that $E^{(i, j)}$ is the $N\times N$ matrix that are all zeros except for the $(i, j)$ entry equals to 1, and $\innerp{\cdot, \cdot}_F$ represents the Frobenius inner product of $\C^{N^2\times N^2}$. Hence 
\begin{equation}
	(\widehat{\mL}\rho^m_{t_l})_{i, j} = \left(\sum_{k = 1}^{N_K}\widehat U_k\rho^m_{t_l}\widehat V_k\right)_{i, j} = \sum_{k = 1}^{N_K}\innerp{P^{m, t_l}_{ij}, \vec(\widehat U_k)\vec(\widehat V_k)^\top}_F = \innerp{P^{m, t_l}_{ij}, \widehat{\bE}}_F
\end{equation}
and $P^{m, t_l}_{ij} = E^{(i, j)} \otimes \rho^m_{t_l}$.
Therefore the loss function can be written with respect to the reshaped unknown matrix $\widehat{\bE}$
\begin{equation}
	\mE(\widehat{\mL}) = \mE(\widehat{\bE}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L\sum_{i, j = 1}^N \abs{(\rho^m_{t_l})_{ij} - \innerp{P^{m, t_l}_{ij}, \widehat{\bE}}_F}^2.
\end{equation}

\subsubsection{Decomposition to Hamiltonian and Kossakowski}
Suppose we have an estimated $\widehat \bE$, we can recover the Hamiltonian and the Kossakowski matrix in the next step. First we use the inverse transformation of $\mR$ to have $\widehat\bL = \mR^{-1}\widehat\bE$, and then by the equation
\begin{equation}
	\widehat\bL \rho = -i[H, \rho] + \frac{1}{2} \sum_{j, k = 1}^{N^2-1}c_{kl}\rbracket{[F_k, \rho F_l^*] + [F_k \rho, F_l^*]}
\end{equation}

\begin{align}
	\widehat\bL\,\vec(\rho) 
	& = \left\{-i\sum_{j = 1}^{N^2-1}h_j(I\otimes F_j - F_j^\top \otimes I) + \sum_{k, l = 1}^{N^2-1} c_{kl}\left[\overline{F_k}\otimes F_l -\frac{1}{2} I\otimes (F_k^\dag F_l) - \frac{1}{2} \overline{(F_k^\dag F_l)}\otimes I\right]\right\}\vec(\rho)(t)\\
	&= \cbracket{\sum_{j = 1}^{N^2-1}h_j\mT(g_j) + \sum_{k, l=  1}^{N^2 - 1}c_{kl}\mT(G_{kl})}\vec(\rho)
\end{align}
%which implies 
%\begin{equation}
%	\widehat \bL = \sum_{j = 1}^{N^2-1}h_j\mT(g_j) + \sum_{k, l=  1}^{N^2 - 1}c_{kl}\mT(G_{kl})
%\end{equation}
Therefore we define the loss function for the decomposition as 
\begin{equation}
	\mE_{H, C}(h, c) = \norm{\widehat{L} - \sum_{j = 1}^{N^2-1}h_j\mT(g_j) + \sum_{k, l=  1}^{N^2 - 1}c_{kl}\mT(G_{kl})}_F^2
\end{equation}
Again by decomposing $C = UV^\dag$ and introducing the constraint $U = V$, we have 
\begin{equation}
	\mE_{H, C}(h, c) = \norm{\widehat{L} - \sum_{j = 1}^{N^2-1}h_j\mT(g_j) + \sum_{k, j, l=  1}^{N^2 - 1}u_{k, j}v^\dag_{j, l}\mT(G_{kl})}_F^2 + \lambda\norm{U - V}_F^2
\end{equation}
Notice that this loss is linear in $h, U, V$, and we optimize using alternating least squares.



\section{Theoretical analysis}
\subsection{Spectrum and derivative estimation}

\bl{We analyze the spectrum/derivative estimation error in terms of measurement number/sample complexity and total Lindbladian simulation time}

\subsection{Lindbladian estimation}




\section{Numerical experiments}

Given a discrete time mesh $t_n = n \Delta t$, generate data $\rho(t_n)$ using the Python package mesolve. Try to learn the quantum channels of $e^{t_n\bL}$ using the ALS code, and then compare the eigenvalues of each result.

\appendix 


\section{temp}
\subsubsection{Trajectory based settings}
Assume that we have $M$ independent density trajectory observed on a uniform discrete mesh $\{t_l\}_{l = 1}^L$, $t_l = l \Delta t$, namely the following data
\begin{equation}
	\textbf{Data:} \{\rho^m_{t_l}\}_{m, l}^{M, L}
\end{equation}
We also assume a Gaussian random noise for each observation point. Our goal is to estimate the Hamiltonian $H$ and the jump operators $C_k$. By the correspondence between $\{H, C_k\}$ and $\{\bV_k, \bU_k\}$, it is equivalent to estimate $\bL$, and therefore, $\mR(\bL)$ from the data. 

We can assume that the initial density $\rho(0)$ follows a distribution $\mu_0$, and then we can define the loss function as
\begin{equation}
	\mE(\widehat{\mL}) = \mathbb{E}_{\mu_0}\left[\int_0^T \norm{\dot{\rho}_t - \widehat{\mL}\rho_t }_F^2 dt\right]
\end{equation}
where $\norm{A}_F$ represents the Frobenius norm of $A$. Given the observation data, it is natural to define the loss that approximates the above integral since it represents the negative log-likelihood of the unknown matrix $\bL$. Namely, we have 
\begin{equation}
	\mE(\widehat{\mL}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L \norm{\dot{\rho}^{m}_{t_l} - \widehat{\mL}\rho^m_{t_l} }_F^2
\end{equation}

In order to use the rearranged low-rank property, we vectorize the density matrix $\rho$ and decompose the Frobenius norm as 
\begin{equation}
	\mE(\widehat{\mL}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L\sum_{i, j = 1}^n \abs{(\dot{\rho}^{m}_{t_l})_{i, j } - (\widehat{\mL}\rho^m_{t_l})_{i, j}}^2.
\end{equation}
Notice that 
\begin{equation}
	(\bU_k\rho \bV_k^\top)_{i, j} = \vec(\bU_k)^\top P_{ij} \vec(\bV_k) = \innerp{P_{ij}, \vec(\bU_k)\vec(\bV_k)^\top}_F
\end{equation}
where $P_{ij} = e_{ij} \otimes \rho$, and $e_{ij}$ is the $n\times n$ matrix that are all zeros except for the $(i, j)$ entry equals to 1, and $\innerp{\cdot, \cdot}_F$ represents the Frobenius inner product of $\R^{n^2\times n^2}$. Hence 
\begin{equation}
	(\widehat{\mL}\rho^m_{t_l})_{i, j} = \left(\sum_{k = 1}^{\widetilde{r}}\bU_k\rho^m_{t_l}\bV_k\right)_{i, j} = \sum_{k = 1}^{\widetilde{r}}\innerp{P^m_{t_l, ij}, \vec(\bU_k)\vec(\bV_k)^\top}_F = \innerp{P^m_{t_l, ij}, \widehat{\bE}}_F
\end{equation}
and $P^m_{t_l, ij} = e_{ij}\otimes \rho^m_{t_l}$.
Therefore the loss function can be written with respect to the reshaped unknown matrix $\widehat{\bE}$
\begin{equation}
	\mE(\widehat{\mL}) = \mE(\widehat{\bE}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L\sum_{i, j = 1}^n \abs{(\rho^m_{t_l})_{ij} - \innerp{P^m_{t_l, ij}, \widehat{\bE}}_F}^2.
\end{equation}

\subsubsection{Connection to Matrix sensing}
In the language of matrix sensing, the matrix $P^m_{t_l, ij}$ can be considered as the measurement matrices, with the corresponding output $(\rho^m_{t_l})_{ij}$. Now we define the operator $\mA : \R^{n^2\times n^2}\to \R^{MLn^2}$ as
\begin{equation}
	\mA(\widehat{\bE}) = \vec\left[(\innerp{P^m_{t_l, ij}, \widehat{\bE}}_F)_{m, l, i, j}\right]
\end{equation}
and the vector $\by\in \R^{MLn^2}$ as $\vec\left[(\rho^m_{t_l})_{ij}\right]$, we have the loss function written as 
\begin{equation}
	\mE(\widehat{E}) = \norm{\mA(\widehat{\bE}) - \by}^2
\end{equation}
up to a constant. 

\subsubsection{Using observables \QL{Not finished yet}}
Assume that we have $M$ independent density trajectory, a set of observables $\{A_q\}_{q = 1}^Q$ so that the density trajectory is observed on a uniform discrete mesh $\{t_l\}_{l = 1}^L$, $t_l = l \Delta t$, namely the following data
\begin{equation}
	\textbf{Data:} \{\expval{A_q}^m (t_l)\}_{m, q, l}^{M, Q, L}
\end{equation}
We also assume a Gaussian random noise for each observation point. Our goal is to estimate the Hamiltonian $H$ and the jump operators $C_k$. By the correspondence between $\{H, C_k\}$ and $\{\bV_k, \bU_k\}$, it is equivalent to estimate $\bL$, and therefore, $\mR(\bL)$ from the data. 

It is natural to define the loss function as 
\begin{equation}
	\mE(\widehat{\bL}) = \frac{1}{Q}\sum_{q = 1}^Q\int_0^T \norm{\expval{A_q}'(t) - 1}
\end{equation}




\section{RIP}

\bl{We analyze the scaling of the Lindbladian reconstruction error in the derivative estimation error}

\subsubsection{Problem settings}
Suppose we observe data from 
\begin{equation}
	y = \mA(M) + z
\end{equation}
where $M$ is an unknown $n_1 \times n_2$ matrix, $\mA : \R^{n_1\times n_2} \to \R^m$ is a linear mapping, and $z$ is an $m$-dimensional noise term. The goal is to recover a good approximation of $M$ while requiring as few measurements as possible. Each measurement is interpreted as 
\begin{equation}
	[\mA(M)]_i = \innerp{A_i, M}_F = y_i
\end{equation}
where $A_i$ is a $n_1\times n_2$ matrix.





We first introduce the isometry constants of the linear map $\mA$. 
\begin{definition}
	For each integer $r = 1, 2, \dots , n$, the isometry constant $\delta_r$ of $\mA$ is the smallest quantity such that 
	\begin{equation}
		(1 - \delta_r)\norm{X}_F^2 \leq \norm{\mA(X)}^2 \leq (1 + \delta_r) \norm{X}_F^2
	\end{equation}
	holds for all matrices $X$ of rank at most $r$. 
\end{definition}
We say that $\mA$ satisfies the RIP at rank $r$ if $\delta_r$ is bounded by a sufficiently small constant between 0 and 1. 



\subsubsection{Gaussian Measurement}
An essential example shows that the Gaussian measurement has RIP.
\begin{definition}
	$\mA$ is a Gaussian measurement operator if each measurement matrix $A_i$, $1 \leq i \leq m$, contains $i.i.d.$ $\mN(0, 1/m)$ entries, and $A_i$ are independent from each other. 
\end{definition}

\begin{theorem}\cite[Theorem 2.3]{candesTightOracleBounds}
	Fix $0 \leq \delta < q$ and let $\mA$ be a random measurement ensemble obeying the following condition: for any given $X \in \R^{n_2 \times n_2}$ and any fixed $0 < t < 1$, 
	\begin{equation}
		\mathbb{P}\left(\abs{\norm{A(X)}^2 - \norm{X}^2_F} > t \norm{X}^2_F\right) \leq C\exp(-cm)
	\end{equation}
	for fixed constants $C, c > 0$ (which may depend on $t$). Then if $m \geq Dnr$, $\mA$ satisfies the RIP with isometry constant $\delta_r \leq \delta$ with probability exceeding $1 - Ce^{-dm}$ for fixed constants $D, d >0$. 
\end{theorem}

If $\mA$ is a Gaussian random measurement ensemble, $\norm{\mA(X)}^2$ is distributed as $m^{-1}\norm{X}^2_F$ times a chi-squared random variable with $m$ degrees of freedom and from standard concentration inequalities, \\
\begin{equation}
	\mathbb{P}\left(\abs{\norm{A(X)}^2 - \norm{X}^2_F} > t \norm{X}^2_F\right) \leq 2\exp(-\frac{m}{2}(t^2/2 - t^3/3))
\end{equation}
Same result holds if $\mA$ is a random projection, or $A_i$ contains sub-Gaussian entries.

\subsubsection{Pauli Measurement}
Let $M\in \C^{n \times n}$ be an unknown matrix of rank at most $r$. Let $W_1, \dots, W_{n^2}$ be an orthonormal basis for $\C^{n \times n}$, with respect to the Frobenius inner product. We choose $m$ basis elements, $S_1, \dots, S_m$, i.i.d. uniformly at random from $\{W_1, \dots, W_{n^2}\}$. We then observe the coefficients $\innerp{S_i, M}$. 

\begin{definition}
	We say the basis $\{W_1, \dots, W_{n^2}\}$ is \textbf{incoherent} if the $W_i$ all have small operator norm, \\
	\begin{equation}
		\norm{W_i} \leq K/\sqrt{n}.
	\end{equation}
	where $K$ is a constant. 
\end{definition}

In the special case that $W_i$ is given by Pauli matrices $P_1 \otimes \cdots \otimes P_n/\sqrt{d}$, they are incoherent with $\norm{W_i} \leq K/\sqrt d$ with $K = 1$. 

The linear operator $\mA$ is constructed by taking the $i$-th measurement as $S_i$. 
\begin{definition}
	For a matrix $X$, its Schatten $p$-norm is defined as 
	\begin{equation}
		\norm{X}_p = \left(\sum_{i}\sigma_i(X)^p\right)^{1/p},
	\end{equation}
	where $\sigma_i(X)$ is the $i$-th singular value of $X$. In particular, $\norm{X}_* = \norm{X}_1$ is the trace norm or nuclear norm. 
\end{definition}

\begin{theorem}\cite[Theorem 2.1]{liuUniversalLowrankMatrix2011}
	Fix some constant $0 \leq \delta <1$. Let $\{W_1, \dots, W_{n^2}\}$ be an orthonormal basis for $\C^{n \times n}$ that is incoherent. Let $m = CK^2\cdot rd \log^6 d$, for some constant $C$ that depends only on $\delta, C = O(1/\delta^2)$. Let $\mA$ be defined as above. Then with high probability over the selection of $\{S_1, \dots, S_m\}$, $\mA$ satisfies the RIP over the set of all $X\in \C^{d \times d}$ such that $\norm{X}_* \leq \sqrt{r} \norm{X}_F$. Furthermore, the failure probability is exponentially small in $\delta^2C$. 
\end{theorem}



\printbibliography

\end{document}