\documentclass[10pt]{article}  % [12pt] option for the benefit of aging markers
\usepackage{amssymb}
\usepackage{amsthm}    % amssymb package contains more mathematical symbols
\usepackage{graphicx}          % graphicx package enables you to paste in graphics
\usepackage{amsmath}         % best maths package on offer
\usepackage{mathtools}


%\mathtoolsset{showonlyrefs}		% Only show the label of equations that have been referenced

\usepackage{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{listings}
\usepackage{titlepic}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{enumerate}
\usepackage{fontspec}
\usepackage{mathalfa}
\usepackage{todonotes}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{showlabels}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{enumitem}% http://ctan.org/pkg/enumitem
\usepackage{braket}
\usepackage{physics}

\usepackage[
backend=bibtex,
style=alphabetic,
isbn=false,
%editor=false,
]{biblatex}
\addbibresource{Lindblad.bib}


\DeclareSourcemap{
  \maps[datatype=bibtex, overwrite]{
    \map{
      \step[fieldset=address, null]
      \step[fieldset=editor, null]
      \step[fieldset=location, null]
    }
  }
}


%\usepackage[style=ieee]{biblatex}
\AtEveryBibitem{%
  	\clearfield{issn} % Remove issn
  	\clearfield{doi} % Remove doi
  	\clearfield{urldate}
	\clearfield{month}
	\clearfield{adress}
  	\ifentrytype{online}{}{% Remove url except for @online
    \clearfield{url}
  }
}
%\renewbibmacro*{url+urldate}{}
%\renewbibmacro*{editor}{}
%\renewbibmacro*{editors}{}
%\renewbibmacro{in:}{}



\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor = blue,
}

\geometry{left = 2.5cm, right=2.5cm, top=1cm, bottom=1.5cm}

%%%%%%%%%%%%%% symbols %%%%%%%%%%%%%
\newcommand{\eps}{\varepsilon}
%%%%%%%%%%%%%%%%%%%% Theorem Environments %%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{Lem}[thm]{Lemma}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{example}[thm]{Example}
\newtheorem{ex}[thm]{Example}
\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%% bold letters %%%%%%%%%
\newcommand{\bSig}{\boldsymbol{\Sigma}}
\newcommand{\bLam}{\boldsymbol{\Lambda}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bdeta}{\boldsymbol{\eta}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bxi}{\boldsymbol{\xi}}
%%%%%%%%%%%%%%%%%% fonts %%%%%%%%%%%%%%%%
\newcommand{\vect}[1] {\pmb{#1}}
\newcommand{\mat}[1]{\pmb{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%%%%%%%%%%%%% brackets %%%%%%%%%%%%%%
\newcommand{\rbracket}[1]{\left(#1\right)}      %round
\newcommand{\sbracket}[1]{\left[#1\right]}      %square    
\newcommand{\cbracket}[1]{\left\{#1\right\}}      %curve
%\newcommand{\norm}[1]{\left\|#1\right\|}
%\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\innerp}[1]{\langle{#1}\rangle}
\newcommand{\dbinnerp}[1]{\langle\hspace{-1mm}\langle{#1}\rangle \hspace{-1mm}\rangle}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
%%%%%%%%%%%%%%%%%%% mathcal %%%%%%%%%%%%%
\def\mH{\mathcal{H}}
\def\mL{\mathcal{L}}
\def\mE{\mathcal{E}}
\def\mS{\mathcal{S}}
\def\mN{\mathcal{N}}
\def\mD{\mathcal{D}}
\def\mA{\mathcal{A}}
\def\mV{\mathcal{V}}
\def\mB{\mathcal{B}}
\def\mR{\mathcal{R}}

%%%%%%%%%%%%%%%%%% words operation %%%%%%%
%\def\supp{\mathrm{supp}}
%\def\span{\mathrm{span}}
\def\div{\mathrm{div}}
%%%%%%%%%%%%%%%%%% mathbb %%%%%%%%%%%%%      
\def\Rn{\mathbb{R}^n}
\def\R{\mathbb{R}}          
\def\C{\mathbb{C}}                       
\def\P{\mathbb{P}}
%%%%%%%%%%%%%%%%% mathbf %%%%%%%%%%%%%%
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bfm}{\mathbf{m}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bDD}{\mathbf{D}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\wbH}{\widetilde{\mathbf{H}}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\boldeta}{\boldsymbol{\eta}}
\newcommand{\boldlambda}{\boldsymbol{\lambda}}
\newcommand{\boldalpha}{\boldsymbol{\alpha}}
\newcommand{\boldxi}{\boldsymbol{\xi}}
\newcommand{\boldphi}{\boldsymbol{\varphi}}
%%%%%%%%%%% text %%%%%%%%%%%%%%%
\newcommand{\Log}{\text{Log}}
\newcommand{\diag}{\text{diag}}
\renewcommand{\vec}{\text{vec}}
%%%%%%%%%%%%%% comments %%%%%%%%%%%%%%%
\newcommand{\QL}[1]{\textcolor{cyan}{{#1}}}
\newcommand{\jl}[1]{\textcolor{red}{[\textsf{JL: #1]}}}
\newcommand{\bl}[1]{{\color{magenta} [\textbf{BL}: #1]}}

% for bwl
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\maf}[1]{\mathfrak{#1}}
\newcommand{\ms}[1]{\mathscr{#1}}
\renewcommand{\R}{\mathbb{R}} 
\renewcommand{\C}{\mathbb{C}} 
\newcommand{\lad}{\lambda}
\newcommand{\si}{\sigma}
\newcommand{\vr}{\varrho}
\newcommand{\rd}{\mathrm{d}}
\newcommand{\magg}[1]{{\color{magenta} #1}}
\def \ww {\omega}
\def \d {\delta}
\def \h {\hat} 
\def \ep {\varepsilon}
\def \w {\widetilde}
\def \q {\quad}
\def \id {{\rm id}}
\def  \mi {{\bf 1}}
\def \bh {\mc{B}(\mc{H})}
\def \l {\langle}
\def \r {\rangle}
\def \si {\sigma}
\def \dd {\cdot}
\def \pa {\partial}


\title{Efficient estimation of Lindbladians from trajectory data}
\author{Quanjun Lang, Bowen Li, Jianfeng Lu}
\date{}

\begin{document}

\maketitle

\tableofcontents
\section{Introduction}

We study the problem of learning in open quantum systems. The goal is to learn the dynamics in the Lindblad quantum master equation (QME)
\begin{equation}\label{eq_QME_main}
	\frac{d}{dt} \rho = -i[H, \rho] + \sum_{k = 1}^r (C_k \rho C_k^\dagger - \frac{1}{2} C_k^\dagger C_k \rho - \frac{1}{2} C_k^\dagger C_k \rho)
\end{equation}
from observations of the trajectories. Here $H$ is the system Hamiltonian and $C_k$'s are jump operators. The density matrix $\rho \in \R^{n \times n}$ here characterizes the distribution of a quantum $\ket{\psi(t)}$ in the sense that $\mathbb{E}[\ketbra{\psi(t)}{\psi(t)}]$. 
Here $\ket{\psi(t)} \in \R^n$ itself satisfies the stochastic Schr\"{o}dinger equation
\begin{equation}
	d \ket{\psi(t)} = \left( -i H \ket{\psi(t)} - \frac{1}{2}\sum_{k = 1}^r C_k^\dagger C_k \ket{\psi(t)}\right) dt + \sum_{k = 1}^r C_k \ket{\psi(t)} dW^k_t
\end{equation}
where $W^k_t$'s are independent Brownian motions.


The above solution of the QME can be written as
\begin{align}\label{eq_QME_Lindbladian}
	\frac{d}{dt} \rho := \mL \rho =  \mL_H\rho + \mL_L \rho, 
\end{align}
where $\mL_H \rho = -i[H, \rho]$ and $\mL_L \rho = \sum_{k = 1}^r (C_k \rho C_k^\dagger - \frac{1}{2} C_k^\dagger C_k \rho - \frac{1}{2} C_k^\dagger C_k \rho)$. The operator $\mL$ is called the Lindbladian superoperators, and solutions of \eqref{eq_QME_Lindbladian} can be expressed using semigroup generated by $\mL$, namely
\begin{equation}
	\rho(t) = e^{\mL t}\rho(0).
\end{equation}

Then the map $\mV(t) = e^{\mL t}$ is a complete positive trace-preserving map for arbitrary time $t$. By the Choi-Kraus' Theorem, a mapping $\mV:\mB(\R^n) \to \mB(\R^n)$ is completely positive and trace-preserving if and only if it can be expressed as 
\begin{equation}
	\mV\rho = \sum_{k}C_k^\dagger \rho C_k
\end{equation}
where $\sum_{k}C_kC_k^\dagger = I_\mH$. 

\subsection*{Related works} 
\bl{Literature review for Lindbladian learning/Prony method for system eigenvalues/Matrix completion}

\subsection*{Difficulty and contribution} 
Firstly, the evaluation of the density requires taking the expectation with respect to an observable, namely
$$ \innerp{A, \rho}_F = \tr(A \rho) = \expval{A}$$
where $A = \sum_i a_i \ketbra{a_i}{a_i}$. When observe using $A$, we obtain state $\ket{a_i}$ with probability $a_i$. In practice, we take the empirical distribution of the observed states as the estimated value of $\tr(A, \rho)$. Therefore, the accuracy suffers from the law of large numbers with order $1/\sqrt{N}$, where $N$ is the number of independent trials. With given observable $A$, the original equation \eqref{eq_QME_main} can be written as 
\begin{equation}
	\frac{d}{dt} \expval{A}(t) = -i\expval{[A, H]}(t) + \sum_{k = 1}^r \left(\expval{C_k^\dagger A C_k}(t) - \frac{1}{2} \expval{A C_k^\dagger C_k} (t) - \frac{1}{2} \expval{C_k^\dagger C_k A}(t) \right)
\end{equation}

Due to the expense of evaluating the states, the derivative term in \eqref{eq_QME_main} is unrealistic to obtain. Since the finite difference 
$$ \frac{\expval{A}(t + \Delta t) - \expval{A}(t)}{\Delta t} $$ 
will amplify the error in $\expval{A}(t)$ and $\expval{A}(t + \Delta t)$. The error in the derivative is of order $\Delta t^2 + \frac{\delta}{\Delta t}$, where the first $\Delta t^2 $ is the error in finite difference using midpoint method, and the $\delta$ is the accuracy in the expectation. In order to achieve $\varepsilon$ accuracy in the derivative, we need $\Delta t \leq \sqrt{\varepsilon}$ and both $\expval{A}(t)$ and $\expval{A}(t + \Delta t)$ to have $\delta = \varepsilon^{3/2}$ accuracy, therefore requires $1/\varepsilon^{3/2}$ number of independent trails in evaluating the expectation.

\subsection*{Notation and outline}
We fix the notation used throughout this work. Let $\mc{H}\simeq \C^N$ be a finite-dimensional Hilbert space with $N = 2^n$ and $\mc{B}(\mc{H})$ be the space of bounded operators. We denote by $\mc{B}_{sa}(\mc{H})$ the subspace of self-adjoint operators and by $\mc{B}^{+}_{sa}(\mc{H})$ the cone of positive semidefinite operators. For simplicity, we write $A \ge 0$ (resp., $A > 0$) for a positive semidefinite (resp., definite) operator. We then denote by $\mc{D}(\mc{H}) := \{\rho \in \mc{B}_{sa}^+(\mc{H})\,;\  \tr \rho =1 \}$ the set of density operators (quantum states), and by $\mc{D}_+(\mc{H})$ the full-rank density operators. Moreover, let $X^*$ be the adjoint operator of $X$ and $\l X, Y\r = \tr (X^*Y)$ be the Hilbert-Schmidt inner product on $\bh$. 
The adjoint of a superoperator $\Phi: \mc{B}(\mc{H}) \to \mc{B}(\mc{H})$ with respect to the inner product $\l \dd,\dd \r $ is denoted by $\Phi^\dag$. 


\section{Preliminary}

\subsection{Quantum Markovian dynamics}

\subsection{Observable and measurements}

\section{A framework of learning Lindbladian}
\bl{We use Prony method to estimate the spectrum of the system and the derivative of the trajectory. We then use the alternating least squares to reconstruct the Lindbladian by exploiting its low-rank structure.
}

\subsection{Prony fitting of the trajectory}
To increase the accuracy of the estimation of the derivatives, we use the Prony method to fit the trajectories $\expval{A}$. Since the solution can be written as $\rho(t) = e^{t\bL } \rho(0)$, and $\bL$ contains the jump operators, it converges to a steady state exponentially. In the case the $\bL$ is diagonalizable, $\bL = U \Sigma U^{-1}$, where $\Sigma = \diag\{\lambda_1, \dots, \lambda_{n^2}\}$(possibly repeated). Then 
\begin{equation}
	\rho(t) = Ue^{t\Sigma}U^{-1}\rho(0)
\end{equation}
\subsection{Alternating least squares}




\subsection{Vectorization and reshaping}
\subsubsection{Vectorization}
Using the fact that 
\begin{equation}
	\vec(AXB) = (B^\top \otimes A)\vec(X)
\end{equation}
where $\vec$ is the vectorization of a matrix in $\R^{n\times n}$  to a vector in $\R^{n^2}$, we can write the vectorized version of the equation \eqref{eq_QME_main}.
\begin{equation}
	\vec(\rho)'(t) = \left\{-i(I\otimes H - H^\top \otimes I) + \sum_{k = 1}^r \left[\overline{C_k}\otimes C_k -\frac{1}{2} I\otimes (C_k^\dag C_k) - \frac{1}{2} \overline{(C_k^\dag C_k)}\otimes I\right]\right\}\vec(\rho)(t) := \bL \ \vec(\rho)(t).
\end{equation}
where the super operator is denoted as $\bL$. Note that $\bL$ has only $2+3r$ Kronecker products in the summation. We can write $\bL$ as 
\begin{equation}
	\bL = \sum_{k = 1}^{\widetilde r} \bV_k \otimes \bU_k
\end{equation}
where $\bV_k$ and $\bU_k$ are matrices of size $n \times n$. And the corresponding operator $\mL$ is written as 
\begin{equation}
	\mL \rho = \sum_{k = 1}^{\widetilde{r}}	\bU_k \rho\bV_k^\top
\end{equation}
Note that $\widetilde{r}$ and $r$ \QL{does not have explicit relationships? According to Bowen, there is a one-to-one relationship between $\{H, C_k\}$ and $\{\bV_k, \bU_k\}$.}

\subsubsection{Reshaping}
We apply the reshaping operator $\mR$, defined as in \cite{vanloanApproximationKroneckerProducts1993}. If matrix $A \in \R^{n^2\times n^2}$, define the rearrangement of $A$ (relative to the blocking parameters $n$) by
\begin{equation}
	A = 
	\begin{bmatrix}
 	A_{11} \dots A_{1n}\\
 	\vdots \ddots \vdots\\
 	A_{n1} \dots A_{nn} 
 	\end{bmatrix}, 
 	\quad
	\mR(A) = 
	\begin{bmatrix}
		\widetilde A_1\\ \vdots \\ \widetilde A_{n}	
	\end{bmatrix}
	,\quad 
	\widetilde A_j = 
	\begin{bmatrix}
		\vec(A_{1j})^\top \\ 
		\vdots \\
		\vec(A_{nj})^\top	
	\end{bmatrix}
\end{equation}
This rearrangement operator $\mR$ gives 
\begin{equation}
	\mR(A\otimes B) = \vec(A) \vec^\top(B),
\end{equation}
Therefore, 
\begin{equation}
	\bE := \mR(\bL) = \sum_{k = 1}^{\widetilde{r}} \vec(\bV_k) \vec^\top(\bU_k).
\end{equation}
That is to say, the matrix $\bE$ has low rank $\widetilde{r}$ compared to $n^2$. 

\subsection{Loss function}
\subsubsection{Trajectory based settings}
Assume that we have $M$ independent density trajectory observed on a uniform discrete mesh $\{t_l\}_{l = 1}^L$, $t_l = l \Delta t$, namely the following data
\begin{equation}
	\textbf{Data:} \{\rho^m_{t_l}\}_{m, l}^{M, L}
\end{equation}
We also assume a Gaussian random noise for each observation point. Our goal is to estimate the Hamiltonian $H$ and the jump operators $C_k$. By the correspondence between $\{H, C_k\}$ and $\{\bV_k, \bU_k\}$, it is equivalent to estimate $\bL$, and therefore, $\mR(\bL)$ from the data. 

We can assume that the initial density $\rho(0)$ follows a distribution $\mu_0$, and then we can define the loss function as
\begin{equation}
	\mE(\widehat{\mL}) = \mathbb{E}_{\mu_0}\left[\int_0^T \norm{\dot{\rho}_t - \widehat{\mL}\rho_t }_F^2 dt\right]
\end{equation}
where $\norm{A}_F$ represents the Frobenius norm of $A$. Given the observation data, it is natural to define the loss that approximates the above integral since it represents the negative log-likelihood of the unknown matrix $\bL$. Namely, we have 
\begin{equation}
	\mE(\widehat{\mL}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L \norm{\dot{\rho}^{m}_{t_l} - \widehat{\mL}\rho^m_{t_l} }_F^2
\end{equation}

In order to use the rearranged low-rank property, we vectorize the density matrix $\rho$ and decompose the Frobenius norm as 
\begin{equation}
	\mE(\widehat{\mL}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L\sum_{i, j = 1}^n \abs{(\dot{\rho}^{m}_{t_l})_{i, j } - (\widehat{\mL}\rho^m_{t_l})_{i, j}}^2.
\end{equation}
Notice that 
\begin{equation}
	(\bU_k\rho \bV_k^\top)_{i, j} = \vec(\bU_k)^\top P_{ij} \vec(\bV_k) = \innerp{P_{ij}, \vec(\bU_k)\vec(\bV_k)^\top}_F
\end{equation}
where $P_{ij} = e_{ij} \otimes \rho$, and $e_{ij}$ is the $n\times n$ matrix that are all zeros except for the $(i, j)$ entry equals to 1, and $\innerp{\cdot, \cdot}_F$ represents the Frobenius inner product of $\R^{n^2\times n^2}$. Hence 
\begin{equation}
	(\widehat{\mL}\rho^m_{t_l})_{i, j} = \left(\sum_{k = 1}^{\widetilde{r}}\bU_k\rho^m_{t_l}\bV_k\right)_{i, j} = \sum_{k = 1}^{\widetilde{r}}\innerp{P^m_{t_l, ij}, \vec(\bU_k)\vec(\bV_k)^\top}_F = \innerp{P^m_{t_l, ij}, \widehat{\bE}}_F
\end{equation}
and $P^m_{t_l, ij} = e_{ij}\otimes \rho^m_{t_l}$.
Therefore the loss function can be written with respect to the reshaped unknown matrix $\widehat{\bE}$
\begin{equation}
	\mE(\widehat{\mL}) = \mE(\widehat{\bE}) = \frac{1}{ML}\sum_{m = 1}^M\sum_{l = 1}^L\sum_{i, j = 1}^n \abs{(\rho^m_{t_l})_{ij} - \innerp{P^m_{t_l, ij}, \widehat{\bE}}_F}^2.
\end{equation}

\subsubsection{Connection to Matrix sensing}
In the language of matrix sensing, the matrix $P^m_{t_l, ij}$ can be considered as the measurement matrices, with the corresponding output $(\rho^m_{t_l})_{ij}$. Now we define the operator $\mA : \R^{n^2\times n^2}\to \R^{MLn^2}$ as
\begin{equation}
	\mA(\widehat{\bE}) = \vec\left[(\innerp{P^m_{t_l, ij}, \widehat{\bE}}_F)_{m, l, i, j}\right]
\end{equation}
and the vector $\by\in \R^{MLn^2}$ as $\vec\left[(\rho^m_{t_l})_{ij}\right]$, we have the loss function written as 
\begin{equation}
	\mE(\widehat{E}) = \norm{\mA(\widehat{\bE}) - \by}^2
\end{equation}
up to a constant. 

\subsubsection{Using observables \QL{Not finished yet}}
Assume that we have $M$ independent density trajectory, a set of observables $\{A_q\}_{q = 1}^Q$ so that the density trajectory is observed on a uniform discrete mesh $\{t_l\}_{l = 1}^L$, $t_l = l \Delta t$, namely the following data
\begin{equation}
	\textbf{Data:} \{\expval{A_q}^m (t_l)\}_{m, q, l}^{M, Q, L}
\end{equation}
We also assume a Gaussian random noise for each observation point. Our goal is to estimate the Hamiltonian $H$ and the jump operators $C_k$. By the correspondence between $\{H, C_k\}$ and $\{\bV_k, \bU_k\}$, it is equivalent to estimate $\bL$, and therefore, $\mR(\bL)$ from the data. 

It is natural to define the loss function as 
\begin{equation}
	\mE(\widehat{\bL}) = \frac{1}{Q}\sum_{q = 1}^Q\int_0^T \norm{\expval{A_q}'(t) - }
\end{equation}

\subsection{Decomposition of the Liouvillian into Hamiltonian and Jump operators }


\section{Theoretical analysis}

\subsection{Spectrum and derivative estimation}

\bl{We analyze the spectrum/derivative estimation error in terms of measurement number/sample complexity and total Lindbladian simulation time}

\subsection{Lindbladian estimation}

\bl{We analyze the scaling of the Lindbladian reconstruction error in the derivative estimation error}

\subsubsection{Problem settings}
Suppose we observe data from 
\begin{equation}
	y = \mA(M) + z
\end{equation}
where $M$ is an unknown $n_1 \times n_2$ matrix, $\mA : \R^{n_1\times n_2} \to \R^m$ is a linear mapping, and $z$ is an $m$-dimensional noise term. The goal is to recover a good approximation of $M$ while requiring as few measurements as possible. Each measurement is interpreted as 
\begin{equation}
	[\mA(M)]_i = \innerp{A_i, M}_F = y_i
\end{equation}
where $A_i$ is a $n_1\times n_2$ matrix.





We first introduce the isometry constants of the linear map $\mA$. 
\begin{definition}
	For each integer $r = 1, 2, \dots , n$, the isometry constant $\delta_r$ of $\mA$ is the smallest quantity such that 
	\begin{equation}
		(1 - \delta_r)\norm{X}_F^2 \leq \norm{\mA(X)}^2 \leq (1 + \delta_r) \norm{X}_F^2
	\end{equation}
	holds for all matrices $X$ of rank at most $r$. 
\end{definition}
We say that $\mA$ satisfies the RIP at rank $r$ if $\delta_r$ is bounded by a sufficiently small constant between 0 and 1. 



\subsubsection{Gaussian Measurement}
An essential example shows that the Gaussian measurement has RIP.
\begin{definition}
	$\mA$ is a Gaussian measurement operator if each measurement matrix $A_i$, $1 \leq i \leq m$, contains $i.i.d.$ $\mN(0, 1/m)$ entries, and $A_i$ are independent from each other. 
\end{definition}

\begin{theorem}\cite[Theorem 2.3]{candesTightOracleBounds}
	Fix $0 \leq \delta < q$ and let $\mA$ be a random measurement ensemble obeying the following condition: for any given $X \in \R^{n_2 \times n_2}$ and any fixed $0 < t < 1$, 
	\begin{equation}
		\mathbb{P}\left(\abs{\norm{A(X)}^2 - \norm{X}^2_F} > t \norm{X}^2_F\right) \leq C\exp(-cm)
	\end{equation}
	for fixed constants $C, c > 0$ (which may depend on $t$). Then if $m \geq Dnr$, $\mA$ satisfies the RIP with isometry constant $\delta_r \leq \delta$ with probability exceeding $1 - Ce^{-dm}$ for fixed constants $D, d >0$. 
\end{theorem}

If $\mA$ is a Gaussian random measurement ensemble, $\norm{\mA(X)}^2$ is distributed as $m^{-1}\norm{X}^2_F$ times a chi-squared random variable with $m$ degrees of freedom and from standard concentration inequalities, \\
\begin{equation}
	\mathbb{P}\left(\abs{\norm{A(X)}^2 - \norm{X}^2_F} > t \norm{X}^2_F\right) \leq 2\exp(-\frac{m}{2}(t^2/2 - t^3/3))
\end{equation}
Same result holds if $\mA$ is a random projection, or $A_i$ contains sub-Gaussian entries.

\subsubsection{Pauli Measurement}
Let $M\in \C^{n \times n}$ be an unknown matrix of rank at most $r$. Let $W_1, \dots, W_{n^2}$ be an orthonormal basis for $\C^{n \times n}$, with respect to the Frobenius inner product. We choose $m$ basis elements, $S_1, \dots, S_m$, i.i.d. uniformly at random from $\{W_1, \dots, W_{n^2}\}$. We then observe the coefficients $\innerp{S_i, M}$. 

\begin{definition}
	We say the basis $\{W_1, \dots, W_{n^2}\}$ is \textbf{incoherent} if the $W_i$ all have small operator norm, \\
	\begin{equation}
		\norm{W_i} \leq K/\sqrt{n}.
	\end{equation}
	where $K$ is a constant. 
\end{definition}

In the special case that $W_i$ is given by Pauli matrices $P_1 \otimes \cdots \otimes P_n/\sqrt{d}$, they are incoherent with $\norm{W_i} \leq K/\sqrt d$ with $K = 1$. 

The linear operator $\mA$ is constructed by taking the $i$-th measurement as $S_i$. 
\begin{definition}
	For a matrix $X$, its Schatten $p$-norm is defined as 
	\begin{equation}
		\norm{X}_p = \left(\sum_{i}\sigma_i(X)^p\right)^{1/p},
	\end{equation}
	where $\sigma_i(X)$ is the $i$-th singular value of $X$. In particular, $\norm{X}_* = \norm{X}_1$ is the trace norm or nuclear norm. 
\end{definition}

\begin{theorem}\cite[Theorem 2.1]{liuUniversalLowrankMatrix2011}
	Fix some constant $0 \leq \delta <1$. Let $\{W_1, \dots, W_{n^2}\}$ be an orthonormal basis for $\C^{n \times n}$ that is incoherent. Let $m = CK^2\cdot rd \log^6 d$, for some constant $C$ that depends only on $\delta, C = O(1/\delta^2)$. Let $\mA$ be defined as above. Then with high probability over the selection of $\{S_1, \dots, S_m\}$, $\mA$ satisfies the RIP over the set of all $X\in \C^{d \times d}$ such that $\norm{X}_* \leq \sqrt{r} \norm{X}_F$. Furthermore, the failure probability is exponentially small in $\delta^2C$. 
\end{theorem}



\section{Numerical experiments}

Given a discrete time mesh $t_n = n \Delta t$, generate data $\rho(t_n)$ using the Python package mesolve. Try to learn the quantum channels of $e^{t_n\bL}$ using the ALS code, and then compare the eigenvalues of each result.



\printbibliography

\end{document}